<!DOCTYPE html>
<html>
<head>
<style>
div.container {
    width: 100%;
    border: 4px solid gray;
}
header, footer {
    padding: 2em;
    color: white;
    background-color: rgb(135, 189, 165);
    clear: left;
    text-align: center;
}
code (
	background-color: blue;
)
nav {
    float: left;
    max-width: 160px;
    margin: 0;
    padding: 1em;
}
nav ul {
    list-style-type: none;
    padding: 0;
}
   
nav ul a {
    text-decoration: none;
}
article {
    border-left: 3px solid gray;
	border-right: 3px solid gray;
	border-top: 3px solid gray;
	border-bottom: 3px solid gray;
    padding-left: 2cm;
	padding-right: 2cm;
    line-height: 1.6;
    overflow: hidden;
}
p {
    font-family:  Verdana, Helvetica, sans-serif, Georgia, Serif;
}
li {
    font-family:  Verdana, Helvetica, sans-serif, Georgia, Serif;
}
    
h1, h2, h3, h4 {
    font-family:  Verdana, Helvetica, sans-serif, Georgia, Serif;
}
table {
    font-family: Verdana, Helvetica, sans-serif, Georgia, Serif;
    border-collapse: collapse;
    width: 50%;
}
td, th {
    border: 1px solid #dddddd;
    text-align: left;
    padding: 8px;
}
tr:nth-child(even) {
    background-color: #dddddd;
}
</style>
</head>
<body>

<div class="container">

<header>
<h1>Boulder 2016 B-cycle Ridership Data Exploration and Predictive Analytics</h1>
</header>

<article>

<h1 id="boulder-2017-b-cycle-ridership-data-exploration-and-predictive-analytics">Boulder 2017 B-cycle Ridership Data Exploration and Predictive Analytics</h1>
<p><a href="http://www.linkedin.com/in/bhasinharpreet">Harpreet Bhasin</a></p>
<p>Feb 20, 2018</p>
<div class="figure" align="center"><img src="figures/Splash.PNG"></div>
<p><a href="https://Boulder.bcycle.com/">Boulder B-cycle</a>is a nonprofit organization formed by Boulder residents to create and operate Boulder’s bike-sharing program on a not-for-profit, financially sustainable basis. Its mission is to &quot;implement and operate a community-supported bike-share program that provides Boulder’s residents, commuters and visitors with an environmentally friendly, financially sustainable, and affordable transportation option that’s ideal for short trips resulting in fewer vehicle miles traveled, less pollution and congestion, more personal mobility and better health and wellness.&quot;</p>
<div class="figure" align="center"><img src="figures/Boulder%20Header.PNG"></div>
<p>Boulder B-cycle posts its trips data set on its website as soon as its annual report is released. Trips data have been available since 2010. The 2017 annual report and its associated dataset for this report were obtained from from <a href="https://boulder.bcycle.com/data-reports">Boulder B-Cycle website</a>.</p>
<div class="figure" align="center"><img src="figures/Boulder%202017%20Annual%20Report.PNG"></div>
<p>This study will explore the publicly available 2017 Boulder B-cycle Trip Data and perform some predictive analytics on the number of bike checkouts using calendar, clock and weather variables as the predictors. The reporting style will follow the <a href=" https://hbhasin.github.io/Boulder-2016-Bike-Share/">Boulder 2016 B-cycle Ridership Data Exploration and Predictive Analytics</a> study to provide continuity and similarity</p>
<p>This study has two parts:</p>
<ol type="1">
	<li><p>Explore the Trips datasets and visualize the data to provide useful and interesting information.</p></li>
	<li><p>Deploy a variety of regression models to train and test the data followed by a prediction on 10 unseen samples.</p></li>
</ol>
<h1 id="part-1-data-exploration">Part 1: Data Exploration</h1>
<h2 id="data-acquisition">Data Acquisition</h2>
<p>Data for this study was downloaded from several sources and combined using the following steps:</p>
<ol type="1">
	<li><p>Downloaded Boulder B-cycle May 2011-January 2018 Trip Data from <a href="https://boulder.bcycle.com/data-reports">Boulder B-Cycle website</a>. The columns names were changed to comply with Python code best practices.</p></li>
	<li><p>Created a list of the 1892 combinations of the 44 checkout/return kiosks. Used <a href="https://developers.google.com/maps/documentation/distance-matrix/">Google Distance Matrix API</a> to provide the bicycling distance and time between each checkout and return kiosk. Google only supports a maximum of 2500 requests a day, it took two days to obtain this data.</p></li>
	<li><p>Obtained daily and hourly weather data via <a href="https://darksky.net/dev/">Dark Sky API</a> for all of 2017. Dark Sky supports up to 1000 requests per day.</p></li>
</ol>
<h3 id="basic-ridership-statistics">Basic Ridership Statistics</h3>
<h4 id="number-of-rides">Number of Rides</h4>
<p>The B-cycle data, as downloaded, contained 122,331 rows of trips data. Under normal circumstances this would mean that 122,331 B-cycle trips were taken in 2017. However, the <a href="http://https://cdn01.bcycle.com/libraries/docs/librariesprovider35/default-document-library/2017-annual-report-final_web.pdf?sfvrsn=996021c5_2">2017 Boulder B-cycle annual report</a> acknowledged 103,568 total trips for the year. The breakdown, totaling 101,146 trips was as follows:</p>
<table>
<thead>
<tr class="header">
<th>Membership Type</th>
<th>Number of Trips</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Annual (Republic Rider)</td>
<td>63,262</td>
</tr>
<tr class="even">
<td>24-hour (Day Tripper)</td>
<td>23,082</td>
</tr>
<tr class="odd">
<td>Maintenance</td>
<td>15,393</td>
</tr>
<tr class="even">
<td>Monthly</td>
<td>14,426</td>
</tr>
<tr class="odd">
<td>7-day</td>
<td>6</td>
</tr>
<tr class="even">
<td><strong>Total Trips</strong></td>
<td><strong>101,146</strong></td>
</tr>
</tbody>
</table>
<p>The Trips dataset reported 39 rows with NaN (Not A Number) entries. Removal of these 39 rows resulted in 122,292 rows with the following breakdown:</p>
<table>
<thead>
<tr class="header">
<th>Membership Type</th>
<th>Number of Trips</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Annual (Republic Rider)</td>
<td>67,092</td>
</tr>
<tr class="even">
<td>24-hour (Day Tripper)</td>
<td>23,082</td>
</tr>
<tr class="odd">
<td>Maintenance</td>
<td>15,393</td>
</tr>
<tr class="even">
<td>Monthly</td>
<td>14,426</td>
</tr>
<tr class="odd">
<td>Pay-per-trip</td>
<td>2,293</td>
</tr>
<tr class="even">
<td>7-day</td>
<td>6</td>
</tr>
<tr class="odd">
<td><strong>Total Trips</strong></td>
<td><strong>122,292</strong></td>
</tr>
</tbody>
</table>
<p>Removing “Maintenance” entries brought the number of rows down to 106,899. There were 1914 entries with a Trip Duration = 0. Removing these entries resulted in 104,985 rows.
Over 1.2% of the Boulder B-cycle rides (1284 rides) had the same checkout station as return station with a trip duration of only 1 minute (Figure 1). It is very likely that the majority of these “rides” are likely people checking out a bike, and then deciding after a very short time that this particular bike doesn’t work for them.</p>
<div class="figure"><img src="figures/Figure%201.PNG"></div>
<p>FIGURE 1: TRIP DURATION WHEN CHECKOUT AND RETURN KIOSKS ARE THE SAME</p>

<p>Removing entries with a Trip Duration = 1 resulted in 103,578 rows with the following breakdown:</p>
<table>
<thead>
<tr class="header">
<th>Membership Type</th>
<th>Number of Trips</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Annual (Republic Rider)</td>
<td>64,842</td>
</tr>
<tr class="even">
<td>24-hour (Day Tripper)</td>
<td>22,466</td>
</tr>
<tr class="odd">
<td>Monthly (People's Pedaler)</td>
<td>14,032</td>
</tr>
<tr class="even">
<td>Pay-per-trip (Casual Cruiser)</td>
<td>2,232</td>
</tr>
<tr class="odd">
<td>7-day</td>
<td>6</td>
</tr>
<tr class="even">
<td><strong>Total Trips</strong></td>
<td><strong>103,578</strong></td>
</tr>
</tbody>
</table>

<p>This number appeared closer to the 103,568 trips reported by Boulder B-cycle although there were differences amongst the individual membership types.</p>
<p>There were 296 rows in the Trips dataset that had “Maintenance” entry in the Return Kiosk column. These 296 rows were removed accordingly.</p>
<p>
Removing the 1,343 rows with a trip duration of 1 minute and 193 rows with invalid kiosk names resulted in <strong>103,282 Boulder B-cycle rides in 2017</strong>.</p>
<table>
<thead>
<tr class="header">
<th>Membership Type</th>
<th>Number of Trips</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Annual (Republic Rider)</td>
<td>64,646</td>
</tr>
<tr class="even">
<td>24-hour (Day Tripper)</td>
<td>22,409</td>
</tr>
<tr class="odd">
<td>Monthly (People's Pedaler)</td>
<td>13,996</td>
</tr>
<tr class="even">
<td>Pay-per-trip (Casual Cruiser)</td>
<td>2,225</td>
</tr>
<tr class="odd">
<td>7-day</td>
<td>6</td>
</tr>
<tr class="even">
<td><strong>Total Trips</strong></td>
<td><strong>103,282</strong></td>
</tr>
</tbody>
</table>
<h2 id="distance-traveled">Distance Traveled</h2>
<p>To estimate the distance between checkout and return kiosks when they are the same, Tyler’s method of using the “average speed of all the other rides (nominal distance ridden divided by the duration), and then applying this average speed to the same-kiosk trip durations” was adopted. This resulted in 158,140 miles ridden in 2017 and sharply contrasted with the 242,004 miles reported by <a href="https://boulder.bcycle.com/data-reports">Boulder B-Cycle website</a>.</p>
<h3 id="most-popular-and-least-popular-checkout-and-return-kiosks">Most Popular and Least Popular Checkout and Return Kiosks</h3>
<h3 id="most-popular">Most Popular</h3>
<p>The following ten kiosks were the most popular checkout kiosks by number of total bike checkouts in 2017.</p>
<table>
<thead>
<tr class="header">
<th>Checkout Kiosk</th>
<th>Number of Checkouts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Folsom &amp; Colorado</td>
<td>4885</td>
</tr>
<tr class="even">
<td>15th &amp; Pearl</td>
<td>4709</td>
</tr>
<tr class="odd">
<td>28th &amp; Mapleton</td>
<td>3666</td>
</tr>
<tr class="even">
<td>Broadway &amp; Euclid</td>
<td>3854</td>
</tr>
<tr class="odd">
<td>21st &amp; Arapahoe</td>
<td>3462</td>
</tr>
<tr class="even">
<td>13th &amp; Spruce</td>
<td>3447</td>
</tr>
<tr class="odd">
<td>Folsom &amp; Pearl</td>
<td>3416</td>
</tr>
<tr class="even">
<td>11th &amp; Pearl</td>
<td>3348</td>
</tr>
<tr class="odd">
<td>19th &amp; Boulder Creek</td>
<td>3185</td>
</tr>
<tr class="even">
<td>13th &amp; Arapahoe</td>
<td>3143</td>
</tr>
</tbody>
</table>
<p>The most popular Checkout Kiosk to Return Kiosk routes were as follows:</p>
<table>
<thead>
<tr class="header">
<th>Checkout Kiosk</th>
<th>Return Kiosk</th>
<th>Number of Trips</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>26th &amp; Pearl</td>
<td>28th &amp; Mapleton</td>
<td>884</td>
</tr>
<tr class="even">
<td>28th &amp; Mapleton</td>
<td>26th &amp; Pearl</td>
<td>883</td>
</tr>
<tr class="odd">
<td>Folsom &amp; Pearl</td>
<td>15th &amp; Pearl</td>
<td>610</td>
</tr>
<tr class="even">
<td>13th &amp; Arapahoe</td>
<td>13th &amp; Arapahoe</td>
<td>610</td>
</tr>
<tr class="odd">
<td>Settlers' Park</td>
<td>11th &amp; Pearl</td>
<td>473</td>
</tr>
<tr class="even">
<td>38th &amp; Arapahoe</td>
<td>48th &amp; Arapahoe</td>
<td>452</td>
</tr>
<tr class="odd">
<td>21st &amp; Arapahoe</td>
<td>21st &amp; Arapahoe</td>
<td>451</td>
</tr>
<tr class="even">
<td>15th &amp; Pearl</td>
<td>Folsom &amp; Pearl</td>
<td>432</td>
</tr>
<tr class="odd">
<td>15th &amp; Pearl</td>
<td>28th &amp; Mapleton</td>
<td>413</td>
</tr>
<tr class="even">
<td>Broadway &amp; Baseline</td>
<td>Williams Village</td>
<td>412</td>
</tr>
</tbody>
</table>
<p>The following ten kiosks were the most popular return kiosks by number of total bike checkouts in 2017.</p>
<table>
<thead>
<tr class="header">
<th>Return Kiosk</th>
<th>Number of Returns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>15th &amp; Pearl</td>
<td>4692</td>
</tr>
<tr class="even">
<td>28th &amp; Mapleton</td>
<td>4282</td>
</tr>
<tr class="odd">
<td>13th &amp; Spruce</td>
<td>4057</td>
</tr>
<tr class="even">
<td>21st &amp; Arapahoe</td>
<td>3943</td>
</tr>
<tr class="odd">
<td>13th &amp; Arapahoe</td>
<td>3708</td>
</tr>
<tr class="even">
<td>Folsom &amp; Pearl</td>
<td>3491</td>
</tr>
<tr class="odd">
<td>11th &amp; Pearl</td>
<td>3464</td>
</tr>
<tr class="even">
<td>48th &amp; Arapahoe</td>
<td>33356</td>
</tr>
<tr class="odd">
<td>The Village</td>
<td>3220</td>
</tr>
<tr class="even">
<td>Twenty Ninth Street North</td>
<td>3156</td>
</tr>
</tbody>
</table>
<h3 id="least-popular">Least Popular</h3>
<p>The following ten kiosks were the least popular checkout kiosks by number of total bike checkouts in 2016.</p>
<table>
<thead>
<tr class="header">
<th>Checkout Kiosk</th>
<th>Number of Checkouts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Broadway &amp; Iris</td>
<td>1328</td>
</tr>
<tr class="even">
<td>13th &amp; College</td>
<td>1299</td>
</tr>
<tr class="odd">
<td>UCAR Center Green</td>
<td>1227</td>
</tr>
<tr class="even">
<td>9th &amp; Pearl</td>
<td>982</td>
</tr>
<tr class="odd">
<td>27th Way &amp; Broadway</td>
<td>678</td>
</tr>
<tr class="even">
<td>33rd &amp; Fisher</td>
<td>648</td>
</tr>
<tr class="odd">
<td>Wilderness Place</td>
<td>370</td>
</tr>
<tr class="even">
<td>30th &amp; Diagonal Highway</td>
<td>247</td>
</tr>
<tr class="odd">
<td>30th &amp; Marine</td>
<td>159</td>
</tr>
<tr class="even">
<td>Gunbarrel North</td>
<td>120</td>
</tr>
</tbody>
</table>
<p>The following ten kiosks were the least popular return kiosks by number of total bike returns in 2016.</p>
<table>
<thead>
<tr class="header">
<th>Return Kiosk</th>
<th>Number of Returns</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Broadway &amp; Iris</td>
<td>1265</td>
</tr>
<tr class="even">
<td>Broadway &amp; University</td>
<td>1070</td>
</tr>
<tr class="odd">
<td>9th &amp; Pearl</td>
<td>963</td>
</tr>
<tr class="even">
<td>13th &amp; College</td>
<td>804</td>
</tr>
<tr class="odd">
<td>33rd &amp; Fisher</td>
<td>708</td>
</tr>
<tr class="even">
<td>27th Way &amp; Broadway</td>
<td>509</td>
</tr>
<tr class="odd">
<td>Wilderness Place</td>
<td>426</td>
</tr>
<tr class="even">
<td>30th &amp; Diagonal Highway</td>
<td>263</td>
</tr>
<tr class="odd">
<td>30th &amp; Marine</td>
<td>170</td>
</tr>
<tr class="even">
<td>Gunbarrel North</td>
<td>114</td>
</tr>
</tbody>
</table>
<h2 id="map-of-station-popularity">Map of Station Popularity</h2>
<h3 id="checkout-kiosks">Checkout Kiosks</h3>
<p>The use of Tableau aided in the creation of the following map showing the popularity of the various Checkout Kiosks (Figure 2). The size of the circle is proportional to the number of checkouts from that kiosk in 2016.</p>
<div class="figure"><img src="figures/Figure%202.PNG"></div>
<p>FIGURE 2: CHECKOUT KIOSK LOCATIONS AND NUMBER OF CHECKOUTS IN 2016</p>
<h3 id="return-kiosks">Return Kiosks</h3>
<p>Similarly, the use of Tableau aided in the creation of the following map showing the popularity of the various Return Kiosks (Figure 3). The size of the circle corresponds to the number of checkouts returned to that kiosk in 2016.</p>
<div class="figure"><img src="figures/Figure%203.PNG"></div>
<p>FIGURE 3: RETURN KIOSK LOCATIONS AND NUMBER OF RETURNS IN 2016</p>
<h2 id="checkouts-per-membership-type">Checkouts per Membership Type</h2>
<p>With the revisions made earlier to the membership type entries, the figure below shows the breakdown:</p>
<table>
<thead>
<tr class="header">
<th>Membership Type</th>
<th>Number of Checkouts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Annual (Republic Rider)</td>
<td>54,610</td>
</tr>
<tr class="even">
<td>24-hour (Day Tripper)</td>
<td>27,889</td>
</tr>
<tr class="odd">
<td>Monthly (People’s Pedaler)</td>
<td>10,549</td>
</tr>
<tr class="even">
<td>Pay-per-trip (Casual Cruiser)</td>
<td>821</td>
</tr>
<tr class="odd">
<td>Semester (150-day)</td>
<td>449</td>
</tr>
<tr class="even">
<td>7-day</td>
<td>1</td>
</tr>
</tbody>
</table>
<p></p>
<p></p>
<p></p>
<div class="figure"><img src="figures/Figure%204.PNG"></div>
<p>FIGURE 4: NUMBER OF CHECKOUTS BY MEMBERSHIP TYPE IN 2016</p>
<h2 id="ridership-by-calendar-and-clock-variables">Ridership by Calendar and Clock Variables</h2>
<h3 id="ridership-by-hour">Ridership by Hour</h3>
<p>Bike checkout time is probably the most important attribute in the Trips dataset. Each checkout time was converted into its integer hour. For example, 7:02 AM or 7:59 AM would be converted to an integer of 7. In this way, total number of checkouts could be aggregated for the year and plotted against their hours of the day, as shown in Figure 5.</p>
<p>It appears that the highest number of checkouts occurred between 4 PM and 5 PM with ridership increasing steadily from 6 AM. Number of checkouts started to decrease after 6 PM.</p>
<div class="figure"><img src="figures/Figure%205.PNG"></div>
<p>FIGURE 5: NUMBER OF CHECKOUTS BY HOUR IN 2016</p>
<p>Figure 6 shows the average distance ridden by the hour of the day in 2016. Interestingly, more distance was covered during the very early hours of the morning (2 AM to 3 AM) period. The typical distance ridden ranged from 1.4 miles to 1.6 miles from 4 AM to 12 midnight.</p>
<div class="figure"><img src="figures/Figure%206.PNG"></div>
<p>FIGURE 6: ESTIMATED AVERAGE MILES RIDDEN BY HOUR OF CHECKOUT IN 2016</p>
<h2 id="ridership-by-hour-and-weekday">Ridership by Hour and Weekday</h2>
<p>Figure 7 shows that weekday ridership patterns are similar. On the other hand weekend ridership demonstrate a busy afternoon (between 12 PM and 3 PM)</p>
<div class="figure"><img src="figures/Figure%207.PNG"></div>
<p>FIGURE 7: CHECKOUTS BY HOUR OF DAY PER WEEKDAY IN 2016</p>
<h2 id="ridership-by-month">Ridership by Month</h2>
<p>Monthly checkouts, as shown in Figure 8, suggest high ridership during the summer months and low ridership during the winter months.</p>
<div class="figure"><img src="figures/Figure%208.PNG"></div>
<p>FIGURE 8: TOTAL CHECKOUTS BY MONTH IN 2016</p>
<h2 id="merging-with-weather">Merging with Weather</h2>
<p>It is highly likely that weather plays a very important role in bike ridership and bike checkout times. This was shown in the previous plots on total checkouts per hour of the day, by weekday, and by month. To verify this, weather data obtained from <a href="https://darksky.net/dev/">Dark Sky API</a> was merged with the Trips dataset and several graphs plotted to visualize the relationships.</p>
<h3 id="checkouts-vs.-daily-temperature">Checkouts vs. Daily Temperature</h3>
<p>Figure 9 shows the total number of checkouts against maximum and minimum daily temperature. It clearly suggests that ridership increased as the temperature increased and vice-versa.</p>
<div class="figure"><img src="figures/Figure%209.PNG"></div>
<p>FIGURE 9: TOTAL CHECKOUTS BY DAILY TEMPERATURE IN 2016</p>
<p>Apparent temperature, as defined by Dark Sky, is “apparent (or “feels like”) temperature in degrees Fahrenheit”. It appeared to have a subtle effect on bike ridership as shown in Figure 10.</p>
<div class="figure"><img src="figures/Figure%2010.PNG"></div>
<p>FIGURE 10: TOTAL CHECKOUTS BY DAILY APPARENT TEMPERATURE IN 2016</p>
<h2 id="checkouts-vs.-daily-cloud-cover">Checkouts vs. Daily Cloud Cover</h2>
<p>Dark Sky defines Cloud Cover as “the percentage of sky occluded by clouds, between 0 and 1, inclusive”. Figure 11 shows the total number of checkouts against daily cloud cover. It clearly suggests that ridership was highest as the cloud cover stayed at around 0.15.</p>
<div class="figure"><img src="figures/Figure%2011.PNG"></div>
<p>FIGURE 11: TOTAL CHECKOUTS BY DAILY CLOUD COVER IN 2016</p>
<h2 id="checkouts-vs.-daily-wind-speed">Checkouts vs. Daily Wind Speed</h2>
<p>Wind speed is reported in miles per hour. As shown in Figure 12, ridership did not seem to be somewhat impacted by higher wind speeds.</p>
<div class="figure"><img src="figures/Figure%2012.PNG"></div>
<p>FIGURE 12: TOTAL CHECKOUTS BY DAILY WIND SPEED IN 2016</p>
<h2 id="checkouts-vs.-daily-humidity">Checkouts vs. Daily Humidity</h2>
<p>Humidity is defined by Dark Sky as “relative humidity, between 0 and 1. Figure 13 shows decreased ridership at higher humidity levels.</p>
<div class="figure"><img src="figures/Figure%2013.PNG"></div>
<p>FIGURE 13: TOTAL CHECKOUTS BY DAILY HUMIDITY IN 2016</p>
<h2 id="checkouts-vs.-daily-visibility">Checkouts vs. Daily Visibility</h2>
<p>Visibility is measured in miles and capped at 10 miles, according to Dark Sky. As Figure 14 shows, ridership peaked when visibility was at 10 miles.</p>
<div class="figure"><img src="figures/Figure%2014.PNG"></div>
<p>FIGURE 14: TOTAL CHECKOUTS BY DAILY VISIBILITY IN 2016</p

<h2 id="days-with-highestlowest-ridership">Days with Highest/Lowest Ridership</h2>
<p>Another interesting data discovery was the fact that Saturdays and Sundays had the highest and lowest ridership depending upon the weather. In his study, Tyler suggests that this may be due to “‘weekend warriors’ who rent B-cycles for pleasure and are highly affected by the weather in their decision to ride”. This may well be the case.</p>
<h3 id="highest-ridership">Highest Ridership</h3>
<table style="width:100%;">
<colgroup>
<col width="20%" />
<col width="22%" />
<col width="17%" />
<col width="17%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th>Checkout Week Day</th>
<th>Date of Checkout</th>
<th>Max Temperature</th>
<th>Min Temperature</th>
<th>Number of Checkouts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Sunday</td>
<td>2016-09-05</td>
<td>86.950</td>
<td>49.210</td>
<td>638</td>
</tr>
<tr class="even">
<td>Tuesday</td>
<td>2016-07-27</td>
<td>87.150</td>
<td>55.300</td>
<td>534</td>
</tr>
<tr class="odd">
<td>Saturday</td>
<td>2016-08-06</td>
<td>76.320</td>
<td>59.090</td>
<td>529</td>
</tr>
<tr class="even">
<td>Thursday</td>
<td>2016-08-05</td>
<td>69.330</td>
<td>57.110</td>
<td>524</td>
</tr>
<tr class="odd">
<td>Thursday</td>
<td>2016-07-29</td>
<td>82.480</td>
<td>58.620</td>
<td>523</td>
</tr>
<tr class="even">
<td>Saturday</td>
<td>2016-08-07</td>
<td>84.200</td>
<td>57.750</td>
<td>512</td>
</tr>
<tr class="odd">
<td>Wednesday</td>
<td>2016-07-21</td>
<td>89.250</td>
<td>61.160</td>
<td>510</td>
</tr>
<tr class="even">
<td>Sunday</td>
<td>2016-08-01</td>
<td>92.090</td>
<td>59.880</td>
<td>509</td>
</tr>
<tr class="odd">
<td>Wednesday</td>
<td>2016-08-04</td>
<td>70.920</td>
<td>58.620</td>
<td>509</td>
</tr>
<tr class="even">
<td>Monday</td>
<td>2016-08-02</td>
<td>80.100</td>
<td>62.820</td>
<td>509</td>
</tr>
</tbody>
</table>
<h3 id="lowest-ridership">Lowest Ridership</h3>
<table style="width:100%;">
<colgroup>
<col width="20%" />
<col width="22%" />
<col width="17%" />
<col width="17%" />
<col width="22%" />
</colgroup>
<thead>
<tr class="header">
<th>Checkout Week Day</th>
<th>Date of Checkout</th>
<th>Max Temperature</th>
<th>Min Temperature</th>
<th>Number of Checkouts</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Friday</td>
<td>2016-01-09</td>
<td>25.800</td>
<td>13.110</td>
<td>33</td>
</tr>
<tr class="even">
<td>Tuesday</td>
<td>2016-12-07</td>
<td>16.050</td>
<td>0.210</td>
<td>32</td>
</tr>
<tr class="odd">
<td>Thursday</td>
<td>2016-03-18</td>
<td>26.860</td>
<td>19.730</td>
<td>30</td>
</tr>
<tr class="even">
<td>Saturday</td>
<td>2016-12-18</td>
<td>18.320</td>
<td>-6.840</td>
<td>26</td>
</tr>
<tr class="odd">
<td>Saturday</td>
<td>2016-04-17</td>
<td>35.670</td>
<td>30.690</td>
<td>24</td>
</tr>
<tr class="even">
<td>Tuesday</td>
<td>2016-03-23</td>
<td>38.610</td>
<td>21.560</td>
<td>24</td>
</tr>
<tr class="odd">
<td>Friday</td>
<td>2016-12-17</td>
<td>6.910</td>
<td>-6.440</td>
<td>21</td>
</tr>
<tr class="even">
<td>Sunday</td>
<td>2016-02-01</td>
<td>27.130</td>
<td>21.230</td>
<td>20</td>
</tr>
<tr class="odd">
<td>Monday</td>
<td>2016-02-02</td>
<td>22.670</td>
<td>11.790</td>
<td>20</td>
</tr>
<tr class="even">
<td>Saturday</td>
<td>2016-12-25</td>
<td>37.090</td>
<td>21.040</td>
<td>11</td>
</tr>
</tbody>
</table>

<h3 id="checkouts-vs.-hourly-weather-variables">Checkouts vs. Hourly Weather Variables</h3>
<p>Hourly weather conditions provide better resolution than daily weather conditions. To investigate this, number of checkouts against hourly weather variables were also plotted and compared with the plots using daily weather variables.</p>

<h3 id="checkouts-vs.-hourly-temperature">Checkouts vs. Hourly Temperature</h3>
<p>The scatter plots in Figure 15 and 16 show that the relationship between the number of checkouts and the hourly temperatures are not linear.</p>

<div class="figure"><img src="figures/Figure%2015.PNG"></div>

<p>FIGURE 15: TOTAL CHECKOUTS BY HOURLY TEMPERATURE IN 2016</p>
<div class="figure"><img src="figures/Figure%2016.PNG"></div>
<p>FIGURE 16: TOTAL CHECKOUTS BY HOURLY APPARENT TEMPERATURE IN 2016</p>
<h3 id="checkouts-vs.-hourly-humidity">Checkouts vs. Hourly Humidity</h3>
<p>Figure 17 shows that humidity affected ridership significantly.</p>
<div class="figure"><img src="figures/Figure%2017.PNG"></div>
<p>FIGURE 17: TOTAL CHECKOUTS BY HOURLY HUMIDITY IN 2016</p>
<h3 id="checkouts-vs.-hourly-cloud-cover">Checkouts vs. Hourly Cloud Cover</h3>
<p>As shown in Figure 18 Cloud Cover certainly impacted ridership.</p>
<div class="figure"><img src="figures/Figure%2018.PNG"></div>
<p>FIGURE 18: TOTAL CHECKOUTS BY HOURLY CLOUD COVER IN 2016</p>
<h3 id="checkouts-vs.-hourly-wind-speed">Checkouts vs. Hourly Wind Speed</h3>
<p>Data on wind speed indicated it was clustered heavily in 0 to 8 miles per hour range, as shown in Figure 19.</p>
<div class="figure"><img src="figures/Figure%2019.PNG"></div>
<p>FIGURE 19: TOTAL CHECKOUTS BY HOURLY WIND SPEED IN 2016</p>
<h3 id="checkouts-vs.-hourly-visibility">Checkouts vs. Hourly Visibility</h3>
<p>As shown in Figure 20 visibility at 10 miles had the greatest impact on ridership.</p>
<div class="figure"><img src="figures/Figure%2020.PNG"></div>
<p>FIGURE 20: TOTAL CHECKOUTS BY HOURLY VISIBILITY IN 2016</p>
<h1 id="part-2-regression-modeling">Part 2: Regression Modeling</h1>
<p>In his study, Tyler attempted to create a linear regression model using a number of calendar and weather variables. Using temperature, temperature squared, humidity, month, weekday, hour of day, holiday and cloud cover as input variables he arrived at an R squared value of 0.7382 which meant that approximately 73.8% of the variation in the hourly ridership could be explained by the selected variables and the linear model he used to fit the data.</p>
<p>In this section various linear and non-linear regression models were used to test and train the Trips data that was merged with the weather data to try to predict the number of checkouts based on calendar, clock and weather conditions.</p>
<p>The following regression models with their brief explanation were used in this study:</p>
<<ul style="list-style-type:disc">
<li><p>Linear Regression</p></li>
<ul style="list-style-type:circle"><li><p>Most widely used statistical and machine learning technique to model relationship between two sets of variables typically using a straight line. Simple to use and fast performance but lacks high accuracy when compared to non-linear models.</p></li></ul>
<li><p>Lasso Regression</p></li>
<ul style="list-style-type:circle"><li><p>A type of linear regression that uses shrinkage to reduce data values toward the mean. Well suited for automating feature selection.</p></li></ul>
<li><p>Ridge Regression<p></li>
<ul style="list-style-type:circle"><li><p>Well suited for data that suffers from multicollinearity, i.e. features with high correlation.</p></li></ul>
<li><p>Bayesian Ridge Regression<p></li>
<ul style="list-style-type:circle"><li><p>An approach to linear regression in which the statistical analysis is undertaken using Bayesian inference.</p></li></ul>
<li><p>Decision Tree Regression<p></li>
<ul style="list-style-type:circle"><li><p>Uses a tree like structure to derive a final decision on the outcome of the analysis.</p></li></ul>
<li><p>Random Forest Regression<p></li>
<ul style="list-style-type:circle"><li><p>An ensemble learning method that operates by constructing a multitude of decision trees to arrive at the mean prediction.</p></li></ul>
<li><p>Extra Trees Regression<p></li>
<ul style="list-style-type:circle"><li><p>An extremely randomized tree regressor. Builds a totally random decision tree.</p></li></ul>
<li><p>Nearest Neighbors Regression<p></li>
<ul style="list-style-type:circle"><li><p>A simple algorithm that uses a similarity measure (e.g. distance between neighbors) to predict the outcome.</p></li></ul>
</ul>
<h2 id="regression-modeling-with-categorical-feature-set">Regression Modeling with Categorical Feature Set</h2>
<p>The Checkout Month, Week Day and Hour numeric variables were converted to categorical features resulting in 44 total features for regression modeling.</p>
<p>Prior to applying the models a feature correlation was performed on all the features to see if any of the features were highly correlated to one another. As shown in Figure 21, Temperature and Apparent Temperature were highly correlated suggesting that one of them could be removed from the features in the model application.</p>
<div class="figure"><img src="figures/Figure%2021.PNG"></div>
<p>FIGURE 21: FEATURE CORRELATIONS</p>
<p>The models used for regression supported the use of several parameters that could be used to adjust or tune them for better performance. In most cases in this study, the parameters were set to default.</p>
<p>The dataset was randomly spilt into 70% for training and 30% for testing. For each model the training and test scores, R Squared and RMSE results were collected and summarized. In addition, the Decision Tree, Random Forest and Extra Trees models also had their Feature Importance bar charts plotted. The chart for Extra Tree model is shown in Figure 22.</p>
<div class="figure"><img src="figures/Figure%2022.PNG"></div>
<p>FIGURE 22: EXTRA TREES REGRESSION MODEL FEATURE IMPORTANCE CHART</p>
<h2 id="regression-modeling-summary-categorical-feature-set">Regression Modeling Summary – Categorical Feature Set</h2>
<table>
<colgroup>
<col width="7%" />
<col width="7%" />
<col width="6%" />
<col width="6%" />
<col width="15%" />
<col width="14%" />
<col width="14%" />
<col width="12%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th>Linear</th>
<th>Lasso</th>
<th>Ridge</th>
<th>Bayesian Ridge</th>
<th>Decision Tree</th>
<th>Random Forest</th>
<th>Extra Trees</th>
<th>Nearest Neighbors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Training Test Score</td>
<td>0.680</td>
<td>0.677</td>
<td>0.677</td>
<td>0.680</td>
<td>1.000</td>
<td>0.943</td>
<td>1.000</td>
<td>0.597</td>
</tr>
<tr class="even">
<td>Test Set Score</td>
<td>0.676</td>
<td>0.674</td>
<td>0.674</td>
<td>0.676</td>
<td>0.423</td>
<td>0.679</td>
<td>0.699</td>
<td>0.496</td>
</tr>
<tr class="odd">
<td>R Squared</td>
<td>0.822253</td>
<td>0.821127</td>
<td>0.821127</td>
<td>0.822232</td>
<td>0.650078</td>
<td>0.824289</td>
<td>0.835802</td>
<td>0.70400</td>
</tr>
<tr class="even">
<td>RMSE</td>
<td>46.268837</td>
<td>46.533272</td>
<td>46.533272</td>
<td>46.273848</td>
<td>82.481079</td>
<td>45.789939</td>
<td>43.059707</td>
<td>72.051003</td>
</tr>
</tbody>
</table>
<p>The Extra Trees regression model achieved the highest accuracy and the lowest RMSE. The Decision Tree model had lowest accuracy and the highest RMSE.</p>
<h2 id="regression-modeling-with-numerical-feature-set">Regression Modeling with Numerical Feature Set</h2>
<p>Using Checkout Month, Week Day and Hour numeric variables resulted in just 9 total features for regression modeling.</p>
<p>Prior to applying the models a feature correlation was performed on all the features to see if any of the features were highly correlated to one another. As shown in Figure 23, Temperature and Apparent Temperature were highly correlated suggesting that one of them could be removed from the features in the model application.</p>
<div class="figure"><img src="figures/Figure%2023.PNG"></div>
<p>FIGURE 23: FEATURE CORRELATION</p>
<p>For each model the training and test scores, R Squared and RMSE results were collected and summarized. In addition, the Decision Tree, Random Forest and Extra Trees models also had their Feature Importance bar charts plotted. The chart for Random Forest and the Extra Trees models are shown in Figures 24 and 25, respectively.</p>
<div class="figure"><img src="figures/Figure%2024.PNG"></div>
<p>FIGURE 24: RANDOM FOREST REGRESSION MODEL FEATURE IMPORTANCE CHART</p>
<div class="figure"><img src="figures/Figure%2025.PNG"></div>
<p>FIGURE 25: EXTRA TREES REGRESSION MODEL FEATURE IMPORTANCE CHART</p>
<h2 id="regression-modeling-summary-numerical-feature-set">Regression Modeling Summary – Numerical Feature Set</h2>
<table>
<colgroup>
<col width="7%" />
<col width="7%" />
<col width="6%" />
<col width="6%" />
<col width="15%" />
<col width="14%" />
<col width="14%" />
<col width="12%" />
<col width="18%" />
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th>Linear</th>
<th>Lasso</th>
<th>Ridge</th>
<th>Bayesian Ridge</th>
<th>Decision Tree</th>
<th>Random Forest</th>
<th>Extra Trees</th>
<th>Nearest Neighbors</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Training Test Score</td>
<td>0.452</td>
<td>0.444</td>
<td>0.444</td>
<td>0.451</td>
<td>1.000</td>
<td>0.947</td>
<td>1.000</td>
<td>0.867</td>
</tr>
<tr class="even">
<td>Test Set Score</td>
<td>0.470</td>
<td>0.463</td>
<td>0.463</td>
<td>0.471</td>
<td>0.481</td>
<td>0.735</td>
<td>0.739</td>
<td>0.616</td>
</tr>
<tr class="odd">
<td>R Squared</td>
<td>0.655919</td>
<td>0.680605</td>
<td>0.680605</td>
<td>0.685989</td>
<td>0.693454</td>
<td>0.857242</td>
<td>0.859465</td>
<td>0.784742</td>
</tr>
<tr class="even">
<td>RMSE</td>
<td>74.533468</td>
<td>75.555613</td>
<td>75.555613</td>
<td>74.519813</td>
<td>73.070383</td>
<td>37.319948</td>
<td>36.782801</td>
<td>54.076287</td>
</tr>
</tbody>
</table>
<h3 id="regression-modeling-summary">Regression Modeling Summary</h3>
<ul>
<li>The data exploration phase of this study revealed the significance of weather variables on the ridership. The regression modeling phase confirmed this to be accurate. Looking at the feature importance graphs generated by the Extra Trees and Random Forest models, the weather attributes rank the highest.</li>
<li>The non-linear regression models performed better than the linear models. In particular, even with a reduced feature set, the non-linear models such as the Extra Trees and the Random Forest were the best performers with R Squared values well above 0.85.</li>
</ul>
<h2 id="testing-regressor-on-unseen-samples">Testing Regressor on unseen samples</h2>
<p>The Extra Trees Forest Regressor with a predictive accuracy of 85.9% was used to predict 10 samples (with numerical feature set) from the dataset that had not been used neither in the training nor in the test sets. The results are tabulated below. The regressor predicted all 10 of the 10 samples accurately.</p>
<table>
<thead>
<tr class="header">
<th>Sample Number</th>
<th>Actual Number of Checkouts</th>
<th>Predicted Number of Checkouts</th>
<th>+/-</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>12</td>
<td>12</td>
<td>0</td>
</tr>
<tr class="even">
<td>2</td>
<td>48</td>
<td>48</td>
<td>0</td>
</tr>
<tr class="odd">
<td>3</td>
<td>9</td>
<td>9</td>
<td>0</td>
</tr>
<tr class="even">
<td>4</td>
<td>33</td>
<td>33</td>
<td>0</td>
</tr>
<tr class="odd">
<td>5</td>
<td>12</td>
<td>12</td>
<td>0</td>
</tr>
<tr class="even">
<td>6</td>
<td>13</td>
<td>13</td>
<td>0</td>
</tr>
<tr class="odd">
<td>7</td>
<td>9</td>
<td>9</td>
<td>0</td>
</tr>
<tr class="even">
<td>8</td>
<td>6</td>
<td>6</td>
<td>0</td>
</tr>
<tr class="odd">
<td>9</td>
<td>8</td>
<td>8</td>
<td>0</td>
</tr>
<tr class="even">
<td>10</td>
<td>5</td>
<td>5</td>
<td>0</td>
</tr>
</tbody>
</table>
<h1 id="part-3-classification-modeling">Part 3: Classification Modeling</h1>
<p>In this section various classification models were used to test and train the Trips data that was merged with the weather data to try to predict the checkout hour based on weather conditions.</p>
<p>The following classification models were used in this study:</p>
<ul style="list-style-type:disc">
<li><p>Linear (Logistic) Classification</p></li>
<ul style="list-style-type:circle"><li><p>Similar to linear regression but used for classification</p></li></ul>
<li>Decision Tree Classification</li>
<ul style="list-style-type:circle"><li><p>Uses a tree like structure to derive at a final decision on the outcome of the analysis</p></li></ul>
<li>Random Forest Classification</li>
<ul style="list-style-type:circle"><li><p>Similar to random forest regression but used for classification</p></li></ul>
<li>Extra Trees Classification</li>
<ul style="list-style-type:circle"><li><p>Similar to extra trees regression but used for classification</p></li></ul>
<li>Naïve Bayes Classification</li>
<ul style="list-style-type:circle"><li><p>Uses the Bayes’ Theorem (i.e. assumes that the presence of a particular feature is unrelated to the presence of any other feature)</p></li></ul>
<li>Gradient Boosting Classification</li>
<ul style="list-style-type:circle"><li><p>A machine learning method that produces a prediction model in the form of an ensemble of weak prediction models, typically decision trees.</p></li></ul>
<li>Nearest Neighbors Classification</li>
<ul style="list-style-type:circle"><li><p>Similar to nearest neighbors regressor but used for classification</p></li></ul>
<li>Multi-layer Perceptron Classification</li>
<ul style="list-style-type:circle"><li><p>A feedforward artificial neural network mode that maps sets of input data onto a set of appropriate outputs.</p></li></ul>
</ul>
<p>The dataset was randomly spilt into 70% for training and 30% for testing. The class labels were defined as follows:</p>
<ul>
<li>Class 0: Number of Checkouts &gt;= 1 and &lt;= 10</li>
<li>Class 1: Number of Checkouts &gt; 10</li>
</ul>
<p>A cross validation using the Stratified Shuffle Split method was performed on the dataset for each model using a training sample size of 50% and a testing sample size of 50% with 10 splits.</p>
<h2 id="classification-modeling-categorical-feature-set">Classification Modeling – Categorical Feature Set</h2>
<p>As in the case of Regression modeling, feature correlation was carried out to determine if any features had a high correlation with one another. As shown in Figure 21, Temperature and Apparent Temperature were highly correlated suggesting that one of them could be removed from the features in the model application.</p>
<p>For each model the training and test scores, Accuracy, F1 (micro), F1 (macro), Precision (macro), Precision (micro), Recall (macro) and Recall (micro) results were collected and summarized. In addition, the Decision Tree, Random Forest and Extra Trees models also had their Feature Importance bar charts plotted.</p>
<h3 id="classification-modeling-summary-categorical-feature-set">Classification Modeling Summary – Categorical Feature Set</h3>
<table style="width:100%;">
<colgroup>
<col width="5%" />
<col width="7%" />
<col width="11%" />
<col width="11%" />
<col width="9%" />
<col width="9%" />
<col width="14%" />
<col width="14%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th>Logistic</th>
<th>Decision Tree</th>
<th>Random Forest</th>
<th>Extra Trees</th>
<th>Naïve Bayes</th>
<th>Nearest Neighbors</th>
<th>Gradient Boosting</th>
<th>Multi-Layer Perceptron</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>0.877010</td>
<td>0.811258</td>
<td>0.856197</td>
<td>0.877010</td>
<td>0.756386</td>
<td>0.714286</td>
<td>0.817408</td>
<td>0.875591</td>
</tr>
<tr class="even">
<td>F1 (macro)</td>
<td>0.877009</td>
<td>0.811243</td>
<td>0.856088</td>
<td>0.876795</td>
<td>0.747293</td>
<td>0.703119</td>
<td>0.817388</td>
<td>0.875557</td>
</tr>
<tr class="odd">
<td>F1 (micro)</td>
<td>0.877010</td>
<td>0.811258</td>
<td>0.856197</td>
<td>0.877010</td>
<td>0.756386</td>
<td>0.714286</td>
<td>0.817408</td>
<td>0.875591</td>
</tr>
<tr class="even">
<td>Precision (macro)</td>
<td>0.877030</td>
<td>0.811341</td>
<td>0.857210</td>
<td>0.877409</td>
<td>0.799952</td>
<td>0.751804</td>
<td>0.817524</td>
<td>0.875965</td>
</tr>
<tr class="odd">
<td>Precision (micro)</td>
<td>0.877010</td>
<td>0.811258</td>
<td>0.856197</td>
<td>0.877010</td>
<td>0.756386</td>
<td>0.714286</td>
<td>0.817408</td>
<td>0.875591</td>
</tr>
<tr class="even">
<td>Recall (macro)</td>
<td>0.8777014</td>
<td>0.811251</td>
<td>0.856172</td>
<td>0.876995</td>
<td>0.756566</td>
<td>0.714103</td>
<td>0.817399</td>
<td>0.875576</td>
</tr>
<tr class="odd">
<td>Recall (micro)</td>
<td>0.877010</td>
<td>0.811258</td>
<td>0.856197</td>
<td>0.877010</td>
<td>0.811258</td>
<td>0.856197</td>
<td>0.877010</td>
<td>0.756386</td>
</tr>
<tr class="even">
<td>F1 (macro)</td>
<td>0.877009</td>
<td>0.811243</td>
<td>0.856088</td>
<td>0.876795</td>
<td>0.747293</td>
<td>0.703119</td>
<td>0.817388</td>
<td>0.875557</td>
</tr>
<tr class="odd">
<td>F1 (micro)</td>
<td>0.877010</td>
<td>0.811258</td>
<td>0.856197</td>
<td>0.877010</td>
<td>0.756386</td>
<td>0.714286</td>
<td>0.817408</td>
<td>0.875591</td>
</tr>
<tr class="even">
<td>Precision (macro)</td>
<td>0.877030</td>
<td>0.811341</td>
<td>0.857210</td>
<td>0.877409</td>
<td>0.799952</td>
<td>0.751804</td>
<td>0.817524</td>
<td>0.875965</td>
</tr>
<tr class="odd">
<td>Precision (micro)</td>
<td>0.877010</td>
<td>0.811258</td>
<td>0.856197</td>
<td>0.877010</td>
<td>0.756386</td>
<td>0.714286</td>
<td>0.817408</td>
<td>0.875591</td>
</tr>
<tr class="even">
<td>Recall (macro)</td>
<td>0.8777014</td>
<td>0.811251</td>
<td>0.856172</td>
<td>0.876995</td>
<td>0.756566</td>
<td>0.714103</td>
<td>0.817399</td>
<td>0.875576</td>
</tr>
<tr class="odd">
<td>Recall (micro)</td>
<td>0.877010</td>
<td>0.811258</td>
<td>0.856197</td>
<td>0.877010</td>
<td>0.756386</td>
<td>0.714286</td>
<td>0.817408</td>
<td>0.875591</td>
</tr>
<tr class="even">
<td>Cross Validation</td>
<td>0.858643</td>
<td>0.804258</td>
<td>0.856145</td>
<td>0.864045</td>
<td>0.740562</td>
<td>0.721885</td>
<td>0.799319</td>
<td>0.870338</td>
</tr>
<tr class="odd">
<td>Execution Time (sec)</td>
<td>10.791598</td>
<td>0.317235</td>
<td>4.023082</td>
<td>3.425960</td>
<td>0.175422</td>
<td>0.994537</td>
<td>48.096231</td>
<td>9.780759</td>
</tr>
</tbody>
</table>
<p>The Extra Trees model attained the highest accuracy in classifying the two classes. The Naïve Bayes model attained the lowest.</p>
<h2 id="classification-modeling-numerical-feature-set">Classification Modeling – Numerical Feature Set</h2>
<p>Using Checkout Month, Week Day and Hour numeric variables resulted in just 9 total features for regression modeling.</p>
<p>As in the case of Regression modeling, feature correlation was carried out to determine if any features had a high correlation with one another. As shown in Figure 26, Temperature and Apparent Temperature were highly correlated suggesting that one of them could be removed from the features in the model application.</p>
<div class="figure">
<img src="figures/Figure%2026.PNG"></div>
<p>FIGURE 26: FEATURE CORRELATION</p>
<p>For each model the training and test scores, Accuracy, F1 (micro), F1 (macro), Precision (macro), Precision (micro), Recall (macro) and Recall (micro) results were collected and summarized. In addition, the Decision Tree, Random Forest, Extra Trees and Gradient Boosting models also had their Feature Importance bar charts plotted. The chart for the Random Forest model is shown in Figure 27.</p>
<div class="figure"><img src="figures/Figure%2027.PNG"></div>
<p>FIGURE 27: RANDOM FOREST CLASSIFICATION MODEL FEATURE IMPORTANCE CHART</p>
<h2 id="classification-modeling-summary-numerical-feature-set">Classification Modeling Summary – Numerical Feature Set</h2>
<table style="width:100%;">
<colgroup>
<col width="5%" />
<col width="7%" />
<col width="11%" />
<col width="11%" />
<col width="9%" />
<col width="9%" />
<col width="14%" />
<col width="14%" />
<col width="17%" />
</colgroup>
<thead>
<tr class="header">
<th>Metric</th>
<th>Logistic</th>
<th>Decision Tree</th>
<th>Random Forest</th>
<th>Extra Trees</th>
<th>Naïve Bayes</th>
<th>Nearest Neighbors</th>
<th>Gradient Boosting</th>
<th>Multi-Layer Perceptron</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Accuracy</td>
<td>0.772942</td>
<td>0.845790</td>
<td>0.878430</td>
<td>0.872280</td>
<td>0.771523</td>
<td>0.815043</td>
<td>0.851939</td>
<td>0.867550</td>
</tr>
<tr class="even">
<td>F1 (macro)</td>
<td>0.772932</td>
<td>0.845788</td>
<td>0.878403</td>
<td>0.872190</td>
<td>0.769910</td>
<td>0.812961</td>
<td>0.851927</td>
<td>0.867515</td>
</tr>
<tr class="odd">
<td>F1 (micro)</td>
<td>0.772942</td>
<td>0.845790</td>
<td>0.878430</td>
<td>0.872280</td>
<td>0.771523</td>
<td>0.815043</td>
<td>0.851939</td>
<td>0.867550</td>
</tr>
<tr class="even">
<td>Precision (macro)</td>
<td>0.773004</td>
<td>0.845802</td>
<td>0.878721</td>
<td>0.873266</td>
<td>0.779530</td>
<td>0.829477</td>
<td>0.852033</td>
<td>0.867893</td>
</tr>
<tr class="odd">
<td>Precision (micro)</td>
<td>0.772942</td>
<td>0.845790</td>
<td>0.878430</td>
<td>0.872280</td>
<td>0.771523</td>
<td>0.815043</td>
<td>0.851939</td>
<td>0.867550</td>
</tr>
<tr class="even">
<td>Recall (macro)</td>
<td>0.772949</td>
<td>0.845790</td>
<td>0.878416</td>
<td>0.872256</td>
<td>0.771603</td>
<td>0.819944</td>
<td>0.851932</td>
<td>0.867535</td>
</tr>
<tr class="odd">
<td>Recall (micro)</td>
<td>0.772942</td>
<td>0.845790</td>
<td>0.878430</td>
<td>0.872280</td>
<td>0.771523</td>
<td>0.815043</td>
<td>0.851939</td>
<td>0.867550</td>
</tr>
<tr class="even">
<td>Cross Validation</td>
<td>0.771672</td>
<td>0.832108</td>
<td>0.874368</td>
<td>0.871019</td>
<td>0.765200</td>
<td>0.822538</td>
<td>0.843486</td>
<td>0.852682</td>
</tr>
<tr class="odd">
<td>Execution Time (sec)</td>
<td>9.676564</td>
<td>0.301543</td>
<td>3.972808</td>
<td>3.246149</td>
<td>0.106590</td>
<td>0.953759</td>
<td>20.225848</td>
<td>4.669855</td>
</tr>
</tbody>
</table>
<p>Both the Random Forest and the Extra Trees classifiers achieved the highest accuracy and the Naïve Bayes the lowest. The cross validation test accuracy were comparable to the F1 (micro), Precision (micro) and the Recall (micro) accuracies.</p>
<h2 id="classification-modeling-summary">Classification Modeling Summary</h2>
<ul>
<li><p>The Extra Trees model attained the highest accuracy in classifying the two classes using the categorical feature set. The Naïve Bayes model attained the lowest.</p></li>
<li><p>The Random Forest Classifier achieved the highest accuracy and the Naïve Bayes the lowest with the numerical feature set.</p></li>
<li><p>The non-linear classification models (except Naive Bayes) performed better than the linear model.</p></li>
</ul>
<h2 id="testing-classifier-on-unseen-samples">Testing Classifier on unseen samples</h2>
<p>The Random Forest Classifier with a predictive accuracy of 87.8% was used to predict 10 samples (with numerical feature set) from the dataset that had not been used neither in the training nor in the test sets. The results are tabulated below. The classifier predicted 9 of the 10 samples accurately. Of the remaining 1 sample, it predicted one class above the actual class.</p>
<table style="width:100%;">
<colgroup>
<col width="14%" />
<col width="27%" />
<col width="13%" />
<col width="30%" />
<col width="13%" />
</colgroup>
<thead>
<tr class="header">
<th>Sample Number</th>
<th>Actual Number of Checkouts</th>
<th>Class Number</th>
<th>Predicted Number of Checkouts</th>
<th>Class Number</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1</td>
<td>Greater than 10</td>
<td>1</td>
<td>Greater than 10</td>
<td>1</td>
</tr>
<tr class="even">
<td>2</td>
<td>Greater than 10</td>
<td>1</td>
<td>Greater than 10</td>
<td>1</td>
</tr>
<tr class="odd">
<td>3</td>
<td>Between 1 and 10</td>
<td>0</td>
<td>Between 1 and 10</td>
<td>0</td>
</tr>
<tr class="even">
<td>4</td>
<td>Greater than 10</td>
<td>1</td>
<td>Greater than 10</td>
<td>1</td>
</tr>
<tr class="odd">
<td>5</td>
<td>Greater than 10</td>
<td>1</td>
<td>Greater than 10</td>
<td>1</td>
</tr>
<tr class="even">
<td>6</td>
<td>Greater than 10</td>
<td>1</td>
<td>Greater than 10</td>
<td>1</td>
</tr>
<tr class="odd">
<td>7</td>
<td>Between 1 and 10</td>
<td>0</td>
<td>Greater than 10</td>
<td>1</td>
</tr>
<tr class="even">
<td>8</td>
<td>Between 1 and 10</td>
<td>0</td>
<td>Between 1 and 10</td>
<td>0</td>
</tr>
<tr class="odd">
<td>9</td>
<td>Between 1 and 10</td>
<td>0</td>
<td>Between 1 and 10</td>
<td>0</td>
</tr>
<tr class="even">
<td>10</td>
<td>Between 1 and 10</td>
<td>0</td>
<td>Between 1 and 10</td>
<td>0</td>
</tr>
</tbody>
</table>
<h1 id="summary">Summary</h1>
<p>This in-depth study on Boulder 2016 Bike Share Trips data was undertaken to continue the work that Tyler started on the 2014 Denver Bike Ridership data. It agrees with his findings that merging calendar, clock and weather attributes into the Trips dataset can reveal ridership patterns and allow regression and classification techniques to be applied for prediction purposes.</p>
<p>This study covered three areas:</p>
<ol style="list-style-type: decimal">
<li>Explored the Trips datasets and visualized the data and provided useful and interesting information.</li>
<li>Deployed a variety of supervised machine learning regression models to predict the number of checkouts using calendar, clock and weather attributes.</li>
<li>Deployed a variety of supervised machine learning classification models to predict two classes reflecting the number of checkouts using calendar, clock and weather attributes.</li>
</ol>
<h2 id="next-steps">Next Steps</h2>
<ul>
<li>Provide and/or present findings to Boulder B-cycle executives to improve future ridership</li>
<li>Develop a simple desktop, web or mobile app that takes calendar, clock and weather variables as inputs and predicts the number of checkouts as the output.</li>
<li>Longmont, CO has just introduced its bike sharing system – this study could be useful to the management.</li>
</ul>
<h2 id="acknowledgements">Acknowledgements</h2>
<p>The original plan was to work only on the Denver 2015 Trips dataset to continue the effort by <a href="https://github.com/tybyers/denver_bcycle">Tyler Byers</a>. Fortunately, the Boulder 2016 dataset became available just in time for this project's undertaking as well. While there are certainly some differences between how the data were analyzed and reported by Boulder B-Cycle and the author, credit must go to <a href="https://boulder.bcycle.com/staff-board">Kevin Crouse</a> for providing the link to Dropbox to download the dataset and his interest in the final report. Credit also goes to my <a href="https://www.springboard.com/">Springboard</a> mentor, <a href="https://www.linkedin.com/in/alexchao56"/>Alex Chao,</a> for his invaluable guidance and feedback on the progress of this project.</p>
</article>

<footer></footer>
</div>

</body>
</html>